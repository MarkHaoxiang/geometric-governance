{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_scatter import scatter_max\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from geometric_governance.util import Logger, RangeOrValue, get_value\n",
    "from geometric_governance.data import (\n",
    "    generate_synthetic_election,\n",
    "    get_scoring_function_winners,\n",
    ")\n",
    "from geometric_governance.model import MessagePassingElectionModel, DeepSetStrategyModel\n",
    "\n",
    "device = torch.device(0) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "config = {\n",
    "    \"num_voters_range\": (3, 50),\n",
    "    \"num_candidates_range\": (2, 10),\n",
    "    \"train_dataset_size\": 1000,\n",
    "    \"train_batch_size\": 128,\n",
    "    \"train_num_epochs\": 20,\n",
    "    \"eval_num_voters\": 75,\n",
    "    \"eval_num_candidates\": 15,\n",
    "    \"eval_dataset_size\": 1_000,\n",
    "    \"welfare_fn\": \"utilitarian\",\n",
    "    \"top_k_candidates\": None,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"use_welfare_loss\": True,\n",
    "    \"use_monotonicity_loss\": True,\n",
    "    \"monotonicity_loss_batch_size\": 32,\n",
    "    \"strategy_loss_batch_size\": 64,\n",
    "}\n",
    "\n",
    "\n",
    "def generate_welfare_bipartite_dataset(\n",
    "    dataset_size: int,\n",
    "    num_voters_range: RangeOrValue,\n",
    "    num_candidates_range: RangeOrValue,\n",
    "    dataloader_batch_size: int,\n",
    "    top_k_candidates: int,\n",
    "    welfare_fn: Literal[\"utilitarian\", \"nash\", \"rawlsian\"],\n",
    "    rng: np.random.Generator,\n",
    "):\n",
    "    graphs = []\n",
    "    for _ in tqdm(range(dataset_size)):\n",
    "        num_voters = get_value(num_voters_range, rng)\n",
    "        num_candidates = get_value(num_candidates_range, rng)\n",
    "\n",
    "        election_data = generate_synthetic_election(\n",
    "            num_voters=num_voters, num_candidates=num_candidates, rng=rng\n",
    "        )\n",
    "\n",
    "        graph = election_data.to_bipartite_graph(top_k_candidates, vote_data=\"ranking\")\n",
    "\n",
    "        candidate_welfare = election_data.voter_utilities\n",
    "        match welfare_fn:\n",
    "            case \"utilitarian\":\n",
    "                candidate_welfare = election_data.voter_utilities.sum(dim=0)\n",
    "            case \"nash\":\n",
    "                candidate_welfare = election_data.voter_utilities.prod(dim=0)\n",
    "            case \"rawlsian\":\n",
    "                candidate_welfare = election_data.voter_utilities.min(dim=0)[0]\n",
    "            case _:\n",
    "                raise ValueError(\"Unknown welfare function.\")\n",
    "\n",
    "        winners = get_scoring_function_winners(candidate_welfare)\n",
    "\n",
    "        graph.y = candidate_welfare\n",
    "        graph.winners = winners\n",
    "        graphs.append(graph)\n",
    "    dataloader = DataLoader(graphs, batch_size=dataloader_batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "train_dataloader = generate_welfare_bipartite_dataset(\n",
    "    dataset_size=config[\"train_dataset_size\"],\n",
    "    num_voters_range=config[\"num_voters_range\"],\n",
    "    num_candidates_range=config[\"num_candidates_range\"],\n",
    "    dataloader_batch_size=config[\"train_batch_size\"],\n",
    "    top_k_candidates=config[\"top_k_candidates\"],\n",
    "    welfare_fn=config[\"welfare_fn\"],\n",
    "    rng=np.random.default_rng(seed=42),\n",
    ")\n",
    "\n",
    "eval_dataloader = generate_welfare_bipartite_dataset(\n",
    "    dataset_size=config[\"eval_dataset_size\"],\n",
    "    num_voters_range=config[\"eval_num_voters\"],\n",
    "    num_candidates_range=config[\"eval_num_candidates\"],\n",
    "    dataloader_batch_size=config[\"train_batch_size\"],\n",
    "    top_k_candidates=config[\"top_k_candidates\"],\n",
    "    welfare_fn=config[\"welfare_fn\"],\n",
    "    rng=np.random.default_rng(seed=16180),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_dim = 1\n",
    "\n",
    "strategy_model = DeepSetStrategyModel(edge_dim=edge_dim, emb_dim=32)\n",
    "election_model = MessagePassingElectionModel(edge_dim=edge_dim)\n",
    "\n",
    "strategy_model.to(device=device)\n",
    "election_model.to(device=device)\n",
    "\n",
    "s_optim = torch.optim.Adam(strategy_model.parameters(), lr=config[\"learning_rate\"])\n",
    "e_optim = torch.optim.Adam(election_model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "experiment_name = \"robust-voting\"\n",
    "logger = Logger(\n",
    "    experiment_name=experiment_name,\n",
    "    config=config,\n",
    "    mode=\"disabled\",\n",
    ")\n",
    "\n",
    "with tqdm(range(config[\"train_num_epochs\"])) as pbar:\n",
    "    for epoch in range(config[\"train_num_epochs\"]):\n",
    "        # Train\n",
    "        train_loss = 0\n",
    "        train_welfare_loss = 0\n",
    "        train_monotonicity_loss = 0\n",
    "        train_welfare = 0\n",
    "\n",
    "        election_model.train()\n",
    "        for data_ in train_dataloader:\n",
    "            data = data_.to(device=device)\n",
    "\n",
    "            # Train strategy model\n",
    "            strategic_votes = strategy_model(\n",
    "                data.edge_attr, data.edge_index, data.candidate_idxs\n",
    "            )\n",
    "\n",
    "            strategy_loss = 0\n",
    "\n",
    "            voters = (~data.candidate_idxs).nonzero()\n",
    "            candidates_to_batch = data.batch[data.candidate_idxs.nonzero()]\n",
    "\n",
    "            perm = torch.randperm(voters.size(0))[: config[\"strategy_loss_batch_size\"]]\n",
    "            for i in perm:\n",
    "                # TODO: We can vectorise this by picking one random vote from each batch\n",
    "                voter_idx = voters[i].item()\n",
    "                batch = data.batch[voter_idx]\n",
    "\n",
    "                voter_edge_attr_index = data.edge_index[0] == voter_idx\n",
    "\n",
    "                gradient_mask = torch.ones(data.edge_attr.shape[0], device=device)\n",
    "                gradient_mask[voter_edge_attr_index] = 0\n",
    "                gradient_mask = gradient_mask.bool()\n",
    "                gradient_mask = gradient_mask.unsqueeze(-1)\n",
    "\n",
    "                # Pass through frozen election model\n",
    "                gradient_cut_strategic_votes = torch.where(\n",
    "                    gradient_mask, strategic_votes.detach(), strategic_votes\n",
    "                )\n",
    "                data_strategy = data.clone()\n",
    "                data_strategy.edge_attr = strategic_votes\n",
    "                out = election_model(data_strategy)[\n",
    "                    (candidates_to_batch == batch).squeeze()\n",
    "                ]\n",
    "\n",
    "                # Calculate loss\n",
    "                vote_probabilities = torch.exp(out)\n",
    "                voter_welfare = data.edge_attr[voter_edge_attr_index].squeeze()\n",
    "\n",
    "                loss = -(vote_probabilities * voter_welfare).sum()\n",
    "                strategy_loss += loss\n",
    "\n",
    "            strategy_loss.backward()\n",
    "            s_optim.step()\n",
    "\n",
    "            # Train election model\n",
    "            e_optim.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                strategic_votes = strategy_model(\n",
    "                    data.edge_attr, data.edge_index, data.candidate_idxs\n",
    "                )\n",
    "            data.edge_attr = strategic_votes\n",
    "            data.edge_attr.requires_grad = True\n",
    "            out = election_model(data)\n",
    "\n",
    "            if config[\"use_welfare_loss\"]:\n",
    "                welfare_loss = (\n",
    "                    -(torch.exp(out) * data.y).sum() / config[\"train_batch_size\"]\n",
    "                )\n",
    "            else:\n",
    "                welfare_loss = -(out * data.winners).sum() / config[\"train_batch_size\"]\n",
    "\n",
    "            # Monotonicity loss\n",
    "            monotonicity_loss = 0\n",
    "\n",
    "            if config[\"use_monotonicity_loss\"]:\n",
    "                candidates = data.candidate_idxs.nonzero()\n",
    "                perm = torch.randperm(candidates.size(0))[\n",
    "                    : config[\"monotonicity_loss_batch_size\"]\n",
    "                ]\n",
    "                for i in perm:\n",
    "                    candidate_idx = candidates[i]\n",
    "                    edge_idxs = data.edge_index[1] == candidate_idx\n",
    "                    grad = torch.autograd.grad(\n",
    "                        outputs=out[i], inputs=data.edge_attr, create_graph=True\n",
    "                    )[0]\n",
    "                    monotonicity_loss += torch.where(\n",
    "                        grad[edge_idxs] < 0,\n",
    "                        -grad[edge_idxs],\n",
    "                        torch.zeros_like(grad[edge_idxs]),\n",
    "                    ).mean()\n",
    "            monotonicity_loss /= config[\"monotonicity_loss_batch_size\"]\n",
    "\n",
    "            loss = welfare_loss + monotonicity_loss\n",
    "            loss.backward()\n",
    "            e_optim.step()\n",
    "\n",
    "            batch_idxs = data.batch[data.candidate_idxs]\n",
    "            _, predicted = scatter_max(out, batch_idxs)\n",
    "            welfare = data.y[predicted].mean()\n",
    "            train_welfare += welfare.item()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_welfare_loss += welfare_loss.item()\n",
    "            if config[\"use_monotonicity_loss\"]:\n",
    "                train_monotonicity_loss += monotonicity_loss.item()\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_welfare_loss /= len(train_dataloader)\n",
    "        train_monotonicity_loss /= len(train_dataloader)\n",
    "        train_welfare /= len(train_dataloader)\n",
    "\n",
    "        logger.log(\n",
    "            {\n",
    "                \"train/total_loss\": train_loss,\n",
    "                \"train/welfare_loss\": train_welfare_loss,\n",
    "                \"train/monotonicity_loss\": train_monotonicity_loss,\n",
    "                \"train/welfare\": train_welfare,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Eval\n",
    "        election_model.eval()\n",
    "        eval_loss = 0\n",
    "        eval_welfare = 0\n",
    "        total, correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for data_ in eval_dataloader:\n",
    "                data = data_.to(device=device)\n",
    "                out = election_model(data)\n",
    "                if config[\"use_welfare_loss\"]:\n",
    "                    welfare_loss = (\n",
    "                        -(torch.exp(out) * data.y).sum() / config[\"train_batch_size\"]\n",
    "                    )\n",
    "                else:\n",
    "                    welfare_loss = (\n",
    "                        -(out * data.winners).sum() / config[\"train_batch_size\"]\n",
    "                    )\n",
    "                batch_idxs = data.batch[data.candidate_idxs]\n",
    "                _, predicted = scatter_max(out, batch_idxs)\n",
    "                _, predicted_ground = scatter_max(data.y, batch_idxs)\n",
    "                total += predicted_ground.shape[0]\n",
    "                correct += (predicted == predicted_ground).sum().item()\n",
    "\n",
    "                welfare = data.y[predicted].mean()\n",
    "                eval_loss += loss.item()\n",
    "                eval_welfare += welfare.item()\n",
    "\n",
    "        eval_loss /= len(eval_dataloader)\n",
    "        eval_welfare /= len(eval_dataloader)\n",
    "        eval_accuracy = correct / total\n",
    "        logger.log(\n",
    "            {\n",
    "                \"eval/loss\": eval_loss,\n",
    "                \"eval/accuracy\": eval_accuracy,\n",
    "                \"eval/welfare\": eval_welfare,\n",
    "            }\n",
    "        )\n",
    "        logger.commit()\n",
    "\n",
    "        pbar.set_postfix(\n",
    "            {\n",
    "                \"train_welfare_loss\": train_welfare_loss,\n",
    "                \"train_monotonicity_loss\": train_monotonicity_loss,\n",
    "                \"train_welfare\": train_welfare,\n",
    "                \"eval_loss\": eval_loss,\n",
    "                \"eval_accuracy\": eval_accuracy,\n",
    "                \"eval_welfare\": eval_welfare,\n",
    "            }\n",
    "        )\n",
    "        pbar.update(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
