{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_scatter import scatter_max\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from geometric_governance.util import Logger, RangeOrValue, get_value\n",
    "from geometric_governance.data import (\n",
    "    generate_synthetic_election,\n",
    "    get_scoring_function_winners,\n",
    ")\n",
    "from geometric_governance.model import MessagePassingElectionModel\n",
    "\n",
    "device = torch.device(0) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "config = {\n",
    "    \"num_voters_range\": (3, 50),\n",
    "    \"num_candidates_range\": (2, 10),\n",
    "    \"train_dataset_size\": 100_000,\n",
    "    \"train_batch_size\": 128,\n",
    "    \"train_num_epochs\": 20,\n",
    "    \"eval_num_voters\": 75,\n",
    "    \"eval_num_candidates\": 15,\n",
    "    \"eval_dataset_size\": 1_000,\n",
    "    \"welfare_fn\": \"utilitarian\",\n",
    "    \"top_k_candidates\": None,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"use_welfare_loss\": True,\n",
    "    \"monotonicity_loss_batch_size\": 32,\n",
    "}\n",
    "\n",
    "\n",
    "def generate_welfare_bipartite_dataset(\n",
    "    dataset_size: int,\n",
    "    num_voters_range: RangeOrValue,\n",
    "    num_candidates_range: RangeOrValue,\n",
    "    dataloader_batch_size: int,\n",
    "    top_k_candidates: int,\n",
    "    welfare_fn: Literal[\"utilitarian\", \"nash\", \"rawlsian\"],\n",
    "    rng: np.random.Generator,\n",
    "):\n",
    "    graphs = []\n",
    "    for _ in tqdm(range(dataset_size)):\n",
    "        num_voters = get_value(num_voters_range, rng)\n",
    "        num_candidates = get_value(num_candidates_range, rng)\n",
    "\n",
    "        election_data = generate_synthetic_election(\n",
    "            num_voters=num_voters, num_candidates=num_candidates, rng=rng\n",
    "        )\n",
    "\n",
    "        graph = election_data.to_bipartite_graph(top_k_candidates, vote_data=\"ranking\")\n",
    "\n",
    "        candidate_welfare = election_data.voter_utilities\n",
    "        match welfare_fn:\n",
    "            case \"utilitarian\":\n",
    "                candidate_welfare = election_data.voter_utilities.sum(dim=0)\n",
    "            case \"nash\":\n",
    "                candidate_welfare = election_data.voter_utilities.prod(dim=0)\n",
    "            case \"rawlsian\":\n",
    "                candidate_welfare = election_data.voter_utilities.min(dim=0)\n",
    "            case _:\n",
    "                raise ValueError(\"Unknown welfare function.\")\n",
    "\n",
    "        winners = get_scoring_function_winners(candidate_welfare)\n",
    "\n",
    "        graph.y = candidate_welfare\n",
    "        graph.winners = winners\n",
    "        graphs.append(graph)\n",
    "    dataloader = DataLoader(graphs, batch_size=dataloader_batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "train_dataloader = generate_welfare_bipartite_dataset(\n",
    "    dataset_size=config[\"train_dataset_size\"],\n",
    "    num_voters_range=config[\"num_voters_range\"],\n",
    "    num_candidates_range=config[\"num_candidates_range\"],\n",
    "    dataloader_batch_size=config[\"train_batch_size\"],\n",
    "    top_k_candidates=config[\"top_k_candidates\"],\n",
    "    welfare_fn=config[\"welfare_fn\"],\n",
    "    rng=np.random.default_rng(seed=42),\n",
    ")\n",
    "\n",
    "eval_dataloader = generate_welfare_bipartite_dataset(\n",
    "    dataset_size=config[\"eval_dataset_size\"],\n",
    "    num_voters_range=config[\"eval_num_voters\"],\n",
    "    num_candidates_range=config[\"eval_num_candidates\"],\n",
    "    dataloader_batch_size=config[\"train_batch_size\"],\n",
    "    top_k_candidates=config[\"top_k_candidates\"],\n",
    "    welfare_fn=config[\"welfare_fn\"],\n",
    "    rng=np.random.default_rng(seed=16180),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MessagePassingElectionModel(edge_dim=1)\n",
    "model.to(device=device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "experiment_name = \"monotonicity-criterion-disabled\"\n",
    "logger = Logger(\n",
    "    experiment_name=experiment_name,\n",
    "    config=config,\n",
    "    mode=\"online\",\n",
    ")\n",
    "\n",
    "with tqdm(range(config[\"train_num_epochs\"])) as pbar:\n",
    "    for epoch in range(config[\"train_num_epochs\"]):\n",
    "        # Train\n",
    "        train_loss = 0\n",
    "        train_welfare_loss = 0\n",
    "        train_monotonicity_loss = 0\n",
    "        train_welfare = 0\n",
    "\n",
    "        model.train()\n",
    "        for data_ in train_dataloader:\n",
    "            optim.zero_grad()\n",
    "\n",
    "            data = data_.to(device=device)\n",
    "            data.edge_attr.requires_grad = True\n",
    "            out = model(data)\n",
    "\n",
    "            if config[\"use_welfare_loss\"]:\n",
    "                welfare_loss = (\n",
    "                    -(torch.exp(out) * data.y).sum() / config[\"train_batch_size\"]\n",
    "                )\n",
    "            else:\n",
    "                welfare_loss = -(out * data.winners).sum() / config[\"train_batch_size\"]\n",
    "\n",
    "            # Monotonicity loss\n",
    "            monotonicity_loss = 0\n",
    "            candidates = data.candidate_idxs.nonzero()\n",
    "            perm = torch.randperm(candidates.size(0))[\n",
    "                : config[\"monotonicity_loss_batch_size\"]\n",
    "            ]\n",
    "            for i in perm:\n",
    "                candidate_idx = candidates[i]\n",
    "                edge_idxs = data.edge_index[1] == candidate_idx\n",
    "                grad = torch.autograd.grad(\n",
    "                    outputs=out[i], inputs=data.edge_attr, create_graph=True\n",
    "                )[0]\n",
    "                monotonicity_loss += torch.where(\n",
    "                    grad[edge_idxs] < 0,\n",
    "                    -grad[edge_idxs],\n",
    "                    torch.zeros_like(grad[edge_idxs]),\n",
    "                ).mean()\n",
    "            monotonicity_loss /= config[\"monotonicity_loss_batch_size\"]\n",
    "\n",
    "            # loss = welfare_loss + monotonicity_loss\n",
    "            loss = welfare_loss\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            batch_idxs = data.batch[data.candidate_idxs]\n",
    "            _, predicted = scatter_max(out, batch_idxs)\n",
    "            welfare = data.y[predicted].mean()\n",
    "            train_welfare += welfare.item()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_welfare_loss += welfare_loss.item()\n",
    "            train_monotonicity_loss += monotonicity_loss.item()\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_welfare_loss /= len(train_dataloader)\n",
    "        train_monotonicity_loss /= len(train_dataloader)\n",
    "        train_welfare /= len(train_dataloader)\n",
    "\n",
    "        logger.log(\n",
    "            {\n",
    "                \"train/total_loss\": train_loss,\n",
    "                \"train/welfare_loss\": train_welfare_loss,\n",
    "                \"train/monotonicity_loss\": train_monotonicity_loss,\n",
    "                \"train/welfare\": train_welfare,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Eval\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        eval_welfare = 0\n",
    "        total, correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for data_ in eval_dataloader:\n",
    "                data = data_.to(device=device)\n",
    "                out = model(data)\n",
    "                if config[\"use_welfare_loss\"]:\n",
    "                    welfare_loss = (\n",
    "                        -(torch.exp(out) * data.y).sum() / config[\"train_batch_size\"]\n",
    "                    )\n",
    "                else:\n",
    "                    welfare_loss = (\n",
    "                        -(out * data.winners).sum() / config[\"train_batch_size\"]\n",
    "                    )\n",
    "                batch_idxs = data.batch[data.candidate_idxs]\n",
    "                _, predicted = scatter_max(out, batch_idxs)\n",
    "                _, predicted_ground = scatter_max(data.y, batch_idxs)\n",
    "                total += predicted_ground.shape[0]\n",
    "                correct += (predicted == predicted_ground).sum().item()\n",
    "\n",
    "                welfare = data.y[predicted].mean()\n",
    "                eval_loss += loss.item()\n",
    "                eval_welfare += welfare.item()\n",
    "\n",
    "        eval_loss /= len(eval_dataloader)\n",
    "        eval_welfare /= len(eval_dataloader)\n",
    "        eval_accuracy = correct / total\n",
    "        logger.log(\n",
    "            {\n",
    "                \"eval/loss\": eval_loss,\n",
    "                \"eval/accuracy\": eval_accuracy,\n",
    "                \"eval/welfare\": eval_welfare,\n",
    "            }\n",
    "        )\n",
    "        logger.commit()\n",
    "\n",
    "        pbar.set_postfix(\n",
    "            {\n",
    "                \"train_welfare_loss\": train_welfare_loss,\n",
    "                \"train_monotonicity_loss\": train_monotonicity_loss,\n",
    "                \"train_welfare\": train_welfare,\n",
    "                \"eval_loss\": eval_loss,\n",
    "                \"eval_accuracy\": eval_accuracy,\n",
    "                \"eval_welfare\": eval_welfare,\n",
    "            }\n",
    "        )\n",
    "        pbar.update(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
